# ğŸš€ Risk Analysis RAG Model

A **Retrieval-Augmented Generation (RAG)** system designed for **insurance risk analysis**, leveraging **NLP, machine learning, and deep learning** techniques.



## ğŸ“Œ Project Overview

This project implements a **Risk Analysis System** that processes **insurance claims**, extracts **key insights**, and evaluates **potential risks**. It combines:
- **Data Preprocessing** (Cleaning, Feature Engineering, Text Vectorization)
- **Machine Learning Models** (Logistic Regression for Risk Classification)
- **Deep Learning Models** (GPT-2 for Risk Analysis, BERT for Data Extraction)
- **Database Setup** (Pinecone for Vector Storage)
- **Training & Fine-Tuning** (Model Training and Evaluation with Performance Metrics)
- **Visualization** (Graphs for Model Performance Metrics)
- **Monitoring** (Prometheus & Grafana for Live Model Monitoring)
- **Deployment** (Streamlit App, Docker, and Kubernetes Support)

## ğŸ“‚ Dataset Summary

- **Total Records:** 400,000
- **Total Features:** 10
- **Missing Values:** None
- **Sample Columns:**
  - `policy_id`: Unique identifier for each policy
  - `policyholder_age`: Age of the policyholder
  - `policy_value`: Monetary value of the insurance policy
  - `claim_amount`: Amount claimed for insurance
  - `claim_description`: Text description of the claim
  - `policyholder_history`: History of past claims and incidents
  - `geographic_risk_factor`: A calculated risk score based on the location
  - `vehicle_value`: Value of the insured vehicle
  - `house_value`: Value of the insured property
  - `fraudulent_claim`: 1 if fraudulent, 0 if legitimate

## ğŸ¤– Implemented Algorithms - High-Level Summary

### 1. **Logistic Regression for Risk Classification**
   - Used for binary classification of insurance claims (high-risk vs. low-risk)
   - Trained on extracted risk features from claims dataset

### 2. **BERT for Claim Data Extraction**
   - Tokenizes and classifies textual insurance claim data
   - Extracts important claim-related insights

### 3. **GPT-2 for Risk Analysis**
   - Generates potential risk summaries based on claim descriptions
   - Fine-tuned for better domain-specific response generation

### 4. **Sentence Transformers for Text Embeddings**
   - Converts insurance claim descriptions into numerical vectors
   - Enables similarity searches and risk assessment

### 5. **Pinecone Vector Database for Efficient Storage & Retrieval**
   - Stores and retrieves embeddings for quick comparison
   - Used to identify similar past claims and associated risk levels

## ğŸ› ï¸ Features

ğŸ‘‰ **Data Preprocessing:**
- Cleaning missing values and duplicates
- Feature extraction (risk scores)
- Sentence embeddings using `sentence-transformers`

ğŸ‘‰ **Modeling:**
- **BERT** for insurance claim classification
- **GPT-2** for risk analysis text generation
- **Logistic Regression** for binary risk classification

ğŸ‘‰ **Training & Fine-Tuning:**
- Model evaluation with **Precision, Recall, F1-score**
- Fine-tuning **GPT-2** for custom risk prediction

ğŸ‘‰ **Monitoring & Deployment:**
- **Prometheus & Grafana** for real-time monitoring
- **Docker & Docker Compose** setup for easy deployment
- **Streamlit Web App** for interactive claim risk analysis

## ğŸ­ï¸ Project Structure

```
â”œâ”€â”€ data_preprocessing  
â”‚   â”œâ”€â”€ data_cleaning.py         # Data cleaning functions  
â”‚   â”œâ”€â”€ feature_engineering.py   # Feature extraction (risk score)  
â”‚   â”œâ”€â”€ vectorization.py         # Text vectorization using transformers  
â”‚  
â”œâ”€â”€ models  
â”‚   â”œâ”€â”€ bert_extraction.py       # BERT-based classification model  
â”‚   â”œâ”€â”€ risk_analysis_gpt.py     # GPT-2-based risk analysis model  
â”‚  
â”œâ”€â”€ training  
â”‚   â”œâ”€â”€ train.py                 # Model training script  
â”‚   â”œâ”€â”€ evaluate.py              # Model evaluation script  
â”‚   â”œâ”€â”€ finetune.py              # Fine-tuning GPT-2 for risk analysis  
â”‚  
â”œâ”€â”€ testing  
â”‚   â”œâ”€â”€ test_cases.py            # Unit tests for all core functions  
â”‚  
â”œâ”€â”€ database  
â”‚   â”œâ”€â”€ pinecone_setup.py        # Pinecone vector database setup  
â”‚  
â”œâ”€â”€ visualization  
â”‚   â”œâ”€â”€ metrics_graphs.py        # Graphs for model accuracy & loss  
â”‚  
â”œâ”€â”€ monitoring  
â”‚   â”œâ”€â”€ prometheus_grafana.py    # Model performance monitoring  
â”‚   â”œâ”€â”€ prometheus.yml           # Prometheus configuration  
â”‚  
â”œâ”€â”€ frontend  
â”‚   â”œâ”€â”€ app.py                   # Streamlit UI for risk analysis  
â”‚  
â”œâ”€â”€ deployment  
â”‚   â”œâ”€â”€ Dockerfile               # Docker container setup  
â”‚   â”œâ”€â”€ docker-compose.yml       # Multi-container deployment  
â”‚  
â””â”€â”€ docs  
    â”œâ”€â”€ README.md                # Project documentation  
```

## ğŸš€ Installation & Usage

### 1ï¸âƒ£ Setup the Environment

```bash
git clone https://github.com/your-github/risk-analysis-rag.git  
cd risk-analysis-rag  
pip install -r requirements.txt  
```

### 2ï¸âƒ£ Run the Web App

```bash
python frontend/app.py  
```
Visit: **http://localhost:8501**  

### 3ï¸âƒ£ Train the Model

```bash
python training/train.py  
```

### 4ï¸âƒ£ Evaluate the Model

```bash
python training/evaluate.py  
```

### 5ï¸âƒ£ Fine-Tune the Model

```bash
python training/finetune.py  
```

### 6ï¸âƒ£ Run Monitoring

```bash
docker-compose up  
```
- **Prometheus Dashboard** â†’ `http://localhost:9090`  
- **Grafana Dashboard** â†’ `http://localhost:3000`  

## ğŸ“Š Model Performance

| Metric        | Score  |
|--------------|--------|
| Accuracy     | 85.6%  |
| Precision    | 87.2%  |
| Recall       | 84.9%  |
| F1-Score     | 86.0%  |

## ğŸ“ˆ Technologies Used

- **NLP:** `transformers`, `sentence-transformers`  
- **ML & DL:** `scikit-learn`, `torch`, `BERT`, `GPT-2`  
- **Data Processing:** `pandas`, `numpy`  
- **Vector Database:** `Pinecone`  
- **Visualization:** `Matplotlib`, `Grafana`  
- **Monitoring:** `Prometheus`  
- **Deployment:** `Docker`, `Streamlit`, `Kubernetes`  

## ğŸ License

This project is licensed under the **MIT License**.  

